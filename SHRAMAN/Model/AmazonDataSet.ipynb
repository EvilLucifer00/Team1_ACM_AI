{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c69248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import joblib\n",
    "import traceback\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6be90603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "import joblib\n",
    "\n",
    "# Define SMAPE function and scorer\n",
    "def smape(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    \"\"\"\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Handle cases where both actual and predicted are zero\n",
    "    denominator = (np.abs(actual) + np.abs(predicted))\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    denominator = np.where(denominator == 0, 1e-10, denominator)\n",
    "    \n",
    "    smape_value = (100 / len(actual)) * np.sum(2 * np.abs(predicted - actual) / denominator)\n",
    "    return smape_value\n",
    "\n",
    "# Create SMAPE scorer for sklearn (lower is better)\n",
    "smape_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: smape(y_true, y_pred),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Feature extraction functions\n",
    "def extract_features_from_catalog(catalog_text):\n",
    "    \"\"\"\n",
    "    Extract structured features from the catalog_content field\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'item_name': '',\n",
    "        'pack_size': '',\n",
    "        'weight': 0.0,\n",
    "        'volume': 0.0,\n",
    "        'count': 0.0,\n",
    "        'unit': '',\n",
    "        'flavor': '',\n",
    "        'brand': '',\n",
    "        'is_organic': 0,\n",
    "        'is_gluten_free': 0,\n",
    "        'is_vegan': 0,\n",
    "        'is_kosher': 0,\n",
    "        'is_sugar_free': 0,\n",
    "        'is_low_carb': 0,\n",
    "        'is_non_gmo': 0,\n",
    "        'calories_per_serving': 0.0,\n",
    "        'protein_content': 0.0,\n",
    "        'fiber_content': 0.0\n",
    "    }\n",
    "    \n",
    "    if pd.isna(catalog_text):\n",
    "        return features\n",
    "    \n",
    "    text = str(catalog_text).lower()\n",
    "    \n",
    "    # Extract item name\n",
    "    item_name_match = re.search(r'item name:\\s*([^\\n]+)', catalog_text, re.IGNORECASE)\n",
    "    if item_name_match:\n",
    "        features['item_name'] = item_name_match.group(1).strip()\n",
    "    \n",
    "    # Extract pack size information\n",
    "    pack_patterns = [\n",
    "        r'pack of\\s*(\\d+)',\n",
    "        r'(\\d+)\\s*count',\n",
    "        r'(\\d+)\\s*pack',\n",
    "        r'(\\d+)\\s*ct'\n",
    "    ]\n",
    "    for pattern in pack_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            features['pack_size'] = match.group(1)\n",
    "            break\n",
    "    \n",
    "    # Extract weight\n",
    "    weight_patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*oz',\n",
    "        r'(\\d+\\.?\\d*)\\s*ounce',\n",
    "        r'(\\d+\\.?\\d*)\\s*lb',\n",
    "        r'(\\d+\\.?\\d*)\\s*pound'\n",
    "    ]\n",
    "    for pattern in weight_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                features['weight'] = float(match.group(1))\n",
    "            except:\n",
    "                features['weight'] = 0.0\n",
    "            break\n",
    "    \n",
    "    # Extract volume\n",
    "    volume_patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*fl\\s*oz',\n",
    "        r'(\\d+\\.?\\d*)\\s*fluid\\s*ounce',\n",
    "        r'(\\d+\\.?\\d*)\\s*ml',\n",
    "        r'(\\d+\\.?\\d*)\\s*liter'\n",
    "    ]\n",
    "    for pattern in volume_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                features['volume'] = float(match.group(1))\n",
    "            except:\n",
    "                features['volume'] = 0.0\n",
    "            break\n",
    "    \n",
    "    # Extract count\n",
    "    count_patterns = [\n",
    "        r'(\\d+)\\s*tea bags',\n",
    "        r'(\\d+)\\s*capsules',\n",
    "        r'(\\d+)\\s*pods',\n",
    "        r'(\\d+)\\s*cookies'\n",
    "    ]\n",
    "    for pattern in count_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                features['count'] = float(match.group(1))\n",
    "            except:\n",
    "                features['count'] = 0.0\n",
    "            break\n",
    "    \n",
    "    # Extract unit from the structured Unit field\n",
    "    unit_match = re.search(r'unit:\\s*([^\\n]+)', catalog_text, re.IGNORECASE)\n",
    "    if unit_match:\n",
    "        features['unit'] = unit_match.group(1).strip().lower()\n",
    "    \n",
    "    # Extract flavor information\n",
    "    flavor_keywords = ['vanilla', 'chocolate', 'strawberry', 'lemon', 'mint', 'berry', \n",
    "                      'caramel', 'honey', 'spice', 'cinnamon', 'ginger', 'peach']\n",
    "    for flavor in flavor_keywords:\n",
    "        if flavor in text:\n",
    "            features['flavor'] = flavor\n",
    "            break\n",
    "    \n",
    "    # Extract brand names\n",
    "    brand_patterns = [\n",
    "        r'manufacturer:\\s*([^\\n]+)',\n",
    "        r'brand:\\s*([^\\n]+)'\n",
    "    ]\n",
    "    for pattern in brand_patterns:\n",
    "        match = re.search(pattern, catalog_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            features['brand'] = match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # Health and dietary attributes\n",
    "    features['is_organic'] = 1 if any(term in text for term in ['organic', 'usda organic']) else 0\n",
    "    features['is_gluten_free'] = 1 if 'gluten free' in text else 0\n",
    "    features['is_vegan'] = 1 if 'vegan' in text else 0\n",
    "    features['is_kosher'] = 1 if 'kosher' in text else 0\n",
    "    features['is_sugar_free'] = 1 if any(term in text for term in ['sugar free', 'no sugar', 'zero sugar']) else 0\n",
    "    features['is_low_carb'] = 1 if any(term in text for term in ['low carb', 'keto', 'keto-friendly']) else 0\n",
    "    features['is_non_gmo'] = 1 if any(term in text for term in ['non-gmo', 'non gmo']) else 0\n",
    "    \n",
    "    # Extract nutritional information\n",
    "    calorie_match = re.search(r'(\\d+)\\s*calories', text)\n",
    "    if calorie_match:\n",
    "        try:\n",
    "            features['calories_per_serving'] = float(calorie_match.group(1))\n",
    "        except:\n",
    "            features['calories_per_serving'] = 0.0\n",
    "    \n",
    "    protein_match = re.search(r'(\\d+\\.?\\d*)\\s*g\\s*protein', text)\n",
    "    if protein_match:\n",
    "        try:\n",
    "            features['protein_content'] = float(protein_match.group(1))\n",
    "        except:\n",
    "            features['protein_content'] = 0.0\n",
    "    \n",
    "    fiber_match = re.search(r'(\\d+\\.?\\d*)\\s*g\\s*fiber', text)\n",
    "    if fiber_match:\n",
    "        try:\n",
    "            features['fiber_content'] = float(fiber_match.group(1))\n",
    "        except:\n",
    "            features['fiber_content'] = 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_derived_features(df, has_price=True):\n",
    "    \"\"\"\n",
    "    Create derived features, handling the case where price might be missing\n",
    "    \"\"\"\n",
    "    # Product category based on item name\n",
    "    def categorize_product(item_name):\n",
    "        if pd.isna(item_name):\n",
    "            return 'other'\n",
    "        item_name = str(item_name).lower()\n",
    "        if any(word in item_name for word in ['tea', 'chai']):\n",
    "            return 'tea'\n",
    "        elif any(word in item_name for word in ['coffee', 'brew']):\n",
    "            return 'coffee'\n",
    "        elif any(word in item_name for word in ['snack', 'chip', 'cracker', 'cookie']):\n",
    "            return 'snack'\n",
    "        elif any(word in item_name for word in ['sauce', 'dressing', 'oil']):\n",
    "            return 'condiment'\n",
    "        elif any(word in item_name for word in ['spice', 'seasoning']):\n",
    "            return 'spice'\n",
    "        elif any(word in item_name for word in ['candy', 'chocolate']):\n",
    "            return 'candy'\n",
    "        elif any(word in item_name for word in ['pasta', 'rice', 'grain']):\n",
    "            return 'grain'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    df['product_category'] = df['item_name'].apply(categorize_product)\n",
    "    \n",
    "    # Text length features\n",
    "    df['catalog_content_length'] = df['catalog_content'].str.len().fillna(0)\n",
    "    \n",
    "    # Only create price-related features if price is available\n",
    "    if has_price and 'price' in df.columns:\n",
    "        if 'weight' in df.columns:\n",
    "            weight_safe = df['weight'].replace(0, np.nan)\n",
    "            df['price_per_oz'] = df['price'] / weight_safe\n",
    "            df['price_per_oz'] = df['price_per_oz'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        if 'volume' in df.columns:\n",
    "            volume_safe = df['volume'].replace(0, np.nan)\n",
    "            df['price_per_fl_oz'] = df['price'] / volume_safe\n",
    "            df['price_per_fl_oz'] = df['price_per_fl_oz'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        if 'count' in df.columns:\n",
    "            count_safe = df['count'].replace(0, np.nan)\n",
    "            df['price_per_count'] = df['price'] / count_safe\n",
    "            df['price_per_count'] = df['price_per_count'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "class RobustSMAPEStackingPricePredictor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.feature_columns = []\n",
    "        self.expected_columns = []\n",
    "        self.stacking_model = None\n",
    "        self.is_trained = False\n",
    "        self.best_params_ = None\n",
    "        \n",
    "    def preprocess_data(self, df, is_training=True):\n",
    "        \"\"\"\n",
    "        Preprocess the data: extract features, handle missing values, encode categorical variables\n",
    "        \"\"\"\n",
    "        print(\"Step 1: Extracting features from catalog content...\")\n",
    "        \n",
    "        # Make a copy to avoid modifying original data\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Extract features\n",
    "        catalog_features = df_processed['catalog_content'].apply(extract_features_from_catalog)\n",
    "        features_df = pd.DataFrame(catalog_features.tolist(), index=df_processed.index)\n",
    "        \n",
    "        # Concatenate with original data\n",
    "        df_enhanced = pd.concat([df_processed, features_df], axis=1)\n",
    "        \n",
    "        # Ensure all expected numerical columns exist with proper defaults\n",
    "        numerical_columns = ['weight', 'volume', 'count', 'calories_per_serving', 'protein_content', 'fiber_content']\n",
    "        for col in numerical_columns:\n",
    "            if col not in df_enhanced.columns:\n",
    "                df_enhanced[col] = 0.0\n",
    "            else:\n",
    "                df_enhanced[col] = pd.to_numeric(df_enhanced[col], errors='coerce').fillna(0.0)\n",
    "        \n",
    "        # Ensure all expected boolean columns exist\n",
    "        boolean_columns = ['is_organic', 'is_gluten_free', 'is_vegan', 'is_kosher', \n",
    "                          'is_sugar_free', 'is_low_carb', 'is_non_gmo']\n",
    "        for col in boolean_columns:\n",
    "            if col not in df_enhanced.columns:\n",
    "                df_enhanced[col] = 0\n",
    "            else:\n",
    "                df_enhanced[col] = df_enhanced[col].astype(int)\n",
    "        \n",
    "        # Handle categorical columns\n",
    "        categorical_columns = ['unit', 'flavor', 'brand', 'item_name']\n",
    "        for col in categorical_columns:\n",
    "            if col not in df_enhanced.columns:\n",
    "                df_enhanced[col] = 'unknown'\n",
    "            else:\n",
    "                df_enhanced[col] = df_enhanced[col].fillna('unknown')\n",
    "        \n",
    "        # Create derived features\n",
    "        has_price = is_training and 'price' in df_enhanced.columns\n",
    "        df_enhanced = create_derived_features(df_enhanced, has_price=has_price)\n",
    "        \n",
    "        # Handle outliers in price if it exists and we're training\n",
    "        if is_training and 'price' in df_enhanced.columns:\n",
    "            Q1 = df_enhanced['price'].quantile(0.25)\n",
    "            Q3 = df_enhanced['price'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = max(0, Q1 - 1.5 * IQR)  # Ensure lower bound is not negative\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            df_enhanced['price'] = np.where(df_enhanced['price'] > upper_bound, upper_bound, df_enhanced['price'])\n",
    "            df_enhanced['price'] = np.where(df_enhanced['price'] < lower_bound, lower_bound, df_enhanced['price'])\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    def prepare_features(self, df, is_training=True):\n",
    "        \"\"\"\n",
    "        Prepare features for modeling with robust error handling\n",
    "        \"\"\"\n",
    "        # Define core features that should always be present\n",
    "        core_features = [\n",
    "            'weight', 'volume', 'count', 'catalog_content_length',\n",
    "            'is_organic', 'is_gluten_free', 'is_vegan', 'is_kosher', \n",
    "            'is_sugar_free', 'is_low_carb', 'is_non_gmo'\n",
    "        ]\n",
    "        \n",
    "        # Add price per unit features if available (only in training)\n",
    "        if is_training:\n",
    "            price_per_features = [col for col in df.columns if col.startswith('price_per')]\n",
    "            feature_columns = core_features + price_per_features\n",
    "        else:\n",
    "            feature_columns = core_features.copy()\n",
    "        \n",
    "        # Ensure all feature columns exist in the dataframe\n",
    "        missing_features = set(feature_columns) - set(df.columns)\n",
    "        for feature in missing_features:\n",
    "            df[feature] = 0.0  # Add missing features with default value\n",
    "        \n",
    "        # Select only the feature columns we want\n",
    "        feature_columns = [col for col in feature_columns if col in df.columns]\n",
    "        X = df[feature_columns].copy()\n",
    "        \n",
    "        # Replace infinite values with NaN\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        if is_training:\n",
    "            self.feature_columns = feature_columns\n",
    "            self.expected_columns = feature_columns.copy()\n",
    "            \n",
    "            # Fit imputer and scaler on training data\n",
    "            X_imputed = self.imputer.fit_transform(X)\n",
    "            \n",
    "            # Scale numerical features\n",
    "            numerical_cols = [col for col in X.columns if X[col].dtype in ['float64', 'float32', 'int64']]\n",
    "            if numerical_cols:\n",
    "                X_imputed = self.scaler.fit_transform(X_imputed)\n",
    "            \n",
    "            X_processed = pd.DataFrame(X_imputed, columns=feature_columns, index=X.index)\n",
    "            \n",
    "            if 'price' in df.columns:\n",
    "                y = df['price'].copy()\n",
    "                return X_processed, y\n",
    "            else:\n",
    "                return X_processed\n",
    "        else:\n",
    "            # Ensure we have the same columns as training\n",
    "            missing_cols = set(self.expected_columns) - set(X.columns)\n",
    "            extra_cols = set(X.columns) - set(self.expected_columns)\n",
    "            \n",
    "            for col in missing_cols:\n",
    "                X[col] = 0.0\n",
    "            for col in extra_cols:\n",
    "                X = X.drop(col, axis=1)\n",
    "            \n",
    "            # Reorder columns to match training\n",
    "            X = X[self.expected_columns]\n",
    "            \n",
    "            # Transform using fitted imputer and scaler\n",
    "            X_imputed = self.imputer.transform(X)\n",
    "            X_imputed = self.scaler.transform(X_imputed)\n",
    "            \n",
    "            X_processed = pd.DataFrame(X_imputed, columns=self.expected_columns, index=X.index)\n",
    "            return X_processed\n",
    "    \n",
    "    def build_stacking_regressor(self):\n",
    "        \"\"\"\n",
    "        Build a StackingRegressor with multiple base models\n",
    "        \"\"\"\n",
    "        # Base models with robust parameters\n",
    "        base_models = [\n",
    "            ('ridge', Ridge(alpha=1.0, random_state=42)),\n",
    "            ('lasso', Lasso(alpha=0.1, random_state=42)),\n",
    "            ('rf', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "            ('gbm', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "            ('svr', SVR(kernel='rbf', C=1.0))\n",
    "        ]\n",
    "        \n",
    "        # Meta-model\n",
    "        meta_model = LinearRegression()\n",
    "        \n",
    "        # Create StackingRegressor\n",
    "        stacking_regressor = StackingRegressor(\n",
    "            estimators=base_models,\n",
    "            final_estimator=meta_model,\n",
    "            cv=5,\n",
    "            passthrough=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return stacking_regressor\n",
    "    \n",
    "    def _calculate_aggregated_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Calculate feature importance by aggregating importances from all base models\n",
    "        \"\"\"\n",
    "        # Initialize importance array\n",
    "        aggregated_importance = np.zeros(len(self.feature_columns))\n",
    "        \n",
    "        # Get importances from each base model\n",
    "        for name, model in self.stacking_model.estimators_:\n",
    "            try:\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    # Tree-based models (RandomForest, GradientBoosting)\n",
    "                    importance = model.feature_importances_\n",
    "                elif hasattr(model, 'coef_'):\n",
    "                    # Linear models (Ridge, Lasso)\n",
    "                    importance = abs(model.coef_)\n",
    "                else:\n",
    "                    # Models without feature importance (SVR) - skip\n",
    "                    continue\n",
    "                \n",
    "                # Ensure the importance array matches our feature dimensions\n",
    "                if len(importance) == len(self.feature_columns):\n",
    "                    aggregated_importance += importance\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not get importance from {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize the importance scores\n",
    "        if aggregated_importance.sum() > 0:\n",
    "            aggregated_importance = aggregated_importance / aggregated_importance.sum()\n",
    "        \n",
    "        # Create DataFrame\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': self.feature_columns,\n",
    "            'importance': aggregated_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return feature_importance_df\n",
    "    \n",
    "    def train(self, train_df, perform_tuning=True):\n",
    "        print(\"Starting SMAPE-tuned StackingRegressor training...\")\n",
    "\n",
    "        # Preprocess training data\n",
    "        df_processed = self.preprocess_data(train_df, is_training=True)\n",
    "\n",
    "        # Prepare features\n",
    "        X, y = self.prepare_features(df_processed, is_training=True)\n",
    "\n",
    "        print(f\"Training data shape: {X.shape}\")\n",
    "        print(f\"Target variable shape: {y.shape}\")\n",
    "        print(f\"Features used: {len(self.feature_columns)}\")\n",
    "\n",
    "        # Split data for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Build stacking regressor\n",
    "        stacking_model = self.build_stacking_regressor()\n",
    "\n",
    "        if perform_tuning:\n",
    "            print(\"Performing randomized hyperparameter tuning with SMAPE scoring...\")\n",
    "\n",
    "            # Define hyperparameter search space\n",
    "            param_distributions = {\n",
    "                'ridge__alpha': np.logspace(-2, 2, 10),\n",
    "                'lasso__alpha': np.logspace(-3, 1, 10),\n",
    "                'rf__n_estimators': np.arange(50, 300, 50),\n",
    "                'rf__max_depth': [None, 5, 10, 15, 20],\n",
    "                'gbm__n_estimators': np.arange(50, 300, 50),\n",
    "                'gbm__learning_rate': np.linspace(0.01, 0.2, 10),\n",
    "                'svr__C': np.logspace(-2, 2, 10),\n",
    "                'svr__gamma': ['scale', 'auto']\n",
    "            }\n",
    "\n",
    "            # Perform RandomizedSearchCV with SMAPE\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=stacking_model,\n",
    "                param_distributions=param_distributions,\n",
    "                n_iter=20,\n",
    "                scoring=smape_scorer,\n",
    "                cv=3,\n",
    "                verbose=2,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            random_search.fit(X_train, y_train)\n",
    "\n",
    "            # Best model and parameters\n",
    "            self.stacking_model = random_search.best_estimator_\n",
    "            self.best_params_ = random_search.best_params_\n",
    "\n",
    "            print(f\"Best parameters found: {self.best_params_}\")\n",
    "\n",
    "        else:\n",
    "            print(\"Training without hyperparameter tuning...\")\n",
    "            self.stacking_model = stacking_model\n",
    "            self.stacking_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model on validation set\n",
    "        y_pred = self.stacking_model.predict(X_val)\n",
    "\n",
    "        val_smape = smape(y_val, y_pred)\n",
    "        val_mae = mean_absolute_error(y_val, y_pred)\n",
    "        val_mse = mean_squared_error(y_val, y_pred)\n",
    "        val_r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "        print(\"\\n=== VALIDATION PERFORMANCE ===\")\n",
    "        print(f\"SMAPE: {val_smape:.4f}%\")\n",
    "        print(f\"MAE: ${val_mae:.2f}\")\n",
    "        print(f\"MSE: ${val_mse:.2f}\")\n",
    "        print(f\"R²: {val_r2:.4f}\")\n",
    "\n",
    "        # Cross-validation SMAPE\n",
    "        cv_scores = cross_val_score(\n",
    "            self.stacking_model, X, y, cv=3, scoring=smape_scorer, n_jobs=-1\n",
    "        )\n",
    "        cv_smape_scores = -cv_scores  # Convert back to positive values\n",
    "\n",
    "        print(f\"\\nCross-validation SMAPE: {cv_smape_scores.mean():.4f}% (+/- {cv_smape_scores.std() * 2:.4f}%)\")\n",
    "\n",
    "        # Retrain on full dataset\n",
    "        print(\"\\nRetraining on full training dataset...\")\n",
    "        self.stacking_model.fit(X, y)\n",
    "        self.is_trained = True\n",
    "\n",
    "        # FIXED: Proper feature importance for stacking regressor\n",
    "        print(\"\\n=== STACKING MODEL ANALYSIS ===\")\n",
    "        \n",
    "        # Show base model weights in the meta-model\n",
    "        if hasattr(self.stacking_model.final_estimator_, 'coef_'):\n",
    "            base_model_names = [name for name, _ in self.stacking_model.estimators_]\n",
    "            meta_coefficients = abs(self.stacking_model.final_estimator_.coef_)\n",
    "            \n",
    "            # Create a DataFrame showing base model importance in the meta-model\n",
    "            meta_importance = pd.DataFrame({\n",
    "                'base_model': base_model_names,\n",
    "                'meta_weight': meta_coefficients\n",
    "            }).sort_values('meta_weight', ascending=False)\n",
    "            \n",
    "            print(\"Base Model Weights in Meta-Model:\")\n",
    "            print(meta_importance)\n",
    "        \n",
    "        # Calculate aggregated feature importance from base models\n",
    "        try:\n",
    "            feature_importance = self._calculate_aggregated_feature_importance()\n",
    "            print(\"\\nTop 10 Most Important Features (Aggregated from Base Models):\")\n",
    "            print(feature_importance.head(10))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nNote: Feature importance calculation skipped: {e}\")\n",
    "\n",
    "        return {\n",
    "            'smape': val_smape,\n",
    "            'mae': val_mae,\n",
    "            'mse': val_mse,\n",
    "            'r2': val_r2,\n",
    "            'cv_smape_mean': cv_smape_scores.mean(),\n",
    "            'cv_smape_std': cv_smape_scores.std()\n",
    "        }\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        \"\"\"\n",
    "        Predict prices for test data using the trained StackingRegressor\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        print(\"Making predictions on test data...\")\n",
    "        \n",
    "        # Preprocess test data\n",
    "        df_processed = self.preprocess_data(test_df, is_training=False)\n",
    "        \n",
    "        # Prepare features\n",
    "        X_test = self.prepare_features(df_processed, is_training=False)\n",
    "        \n",
    "        print(f\"Test data shape after preprocessing: {X_test.shape}\")\n",
    "        print(f\"Expected features: {len(self.expected_columns)}\")\n",
    "        print(f\"Actual features: {len(X_test.columns)}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.stacking_model.predict(X_test)\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results = test_df.copy()\n",
    "        results['predicted_price'] = predictions\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model and preprocessors\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"No trained model to save\")\n",
    "        \n",
    "        model_data = {\n",
    "            'scaler': self.scaler,\n",
    "            'imputer': self.imputer,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'expected_columns': self.expected_columns,\n",
    "            'stacking_model': self.stacking_model,\n",
    "            'best_params': self.best_params_,\n",
    "            'is_trained': self.is_trained\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"SMAPE-tuned StackingRegressor saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Load a trained model and preprocessors\n",
    "        \"\"\"\n",
    "        model_data = joblib.load(filepath)\n",
    "        \n",
    "        self.scaler = model_data['scaler']\n",
    "        self.imputer = model_data['imputer']\n",
    "        self.feature_columns = model_data['feature_columns']\n",
    "        self.expected_columns = model_data['expected_columns']\n",
    "        self.stacking_model = model_data['stacking_model']\n",
    "        self.best_params_ = model_data['best_params']\n",
    "        self.is_trained = model_data['is_trained']\n",
    "        \n",
    "        print(f\"SMAPE-tuned StackingRegressor loaded from {filepath}\")\n",
    "        if self.best_params_:\n",
    "            print(f\"Best parameters: {self.best_params_}\")\n",
    "\n",
    "# Example usage function\n",
    "def run_complete_pipeline(train_df, test_df, perform_tuning=True):\n",
    "    \"\"\"\n",
    "    Complete pipeline for training and prediction\n",
    "    \"\"\"\n",
    "    print(\"=== AMAZON PRODUCT PRICE PREDICTION PIPELINE ===\")\n",
    "    \n",
    "    # Initialize predictor\n",
    "    smape_predictor = RobustSMAPEStackingPricePredictor()\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        training_results = smape_predictor.train(train_df, perform_tuning=perform_tuning)\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions = smape_predictor.predict(test_df)\n",
    "        \n",
    "        print(\"\\n=== PREDICTION COMPLETE ===\")\n",
    "        print(f\"Predicted prices for {len(test_predictions)} test samples\")\n",
    "        print(f\"Price range: ${test_predictions['predicted_price'].min():.2f} - ${test_predictions['predicted_price'].max():.2f}\")\n",
    "        \n",
    "        return smape_predictor, training_results, test_predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in pipeline: {e}\")\n",
    "        import traceback\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc375c86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'your_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Assuming these functions are defined elsewhere in your code\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myour_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_features_from_catalog, create_derived_features, smape, smape_scorer\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRobustSMAPEStackingPricePredictor\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'your_utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "import joblib\n",
    "\n",
    "# Define SMAPE function and scorer\n",
    "def smape(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    \"\"\"\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Handle cases where both actual and predicted are zero\n",
    "    denominator = (np.abs(actual) + np.abs(predicted))\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    denominator = np.where(denominator == 0, 1e-10, denominator)\n",
    "    \n",
    "    smape_value = (100 / len(actual)) * np.sum(2 * np.abs(predicted - actual) / denominator)\n",
    "    return smape_value\n",
    "\n",
    "# Create SMAPE scorer for sklearn (lower is better)\n",
    "smape_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: smape(y_true, y_pred),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Feature extraction functions\n",
    "def extract_features_from_catalog(catalog_text):\n",
    "    \"\"\"\n",
    "    Extract structured features from the catalog_content field\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'item_name': '',\n",
    "        'pack_size': '',\n",
    "        'weight': 0.0,\n",
    "        'volume': 0.0,\n",
    "        'count': 0.0,\n",
    "        'unit': '',\n",
    "        'flavor': '',\n",
    "        'brand': '',\n",
    "        'is_organic': 0,\n",
    "        'is_gluten_free': 0,\n",
    "        'is_vegan': 0,\n",
    "        'is_kosher': 0,\n",
    "        'is_sugar_free': 0,\n",
    "        'is_low_carb': 0,\n",
    "        'is_non_gmo': 0,\n",
    "        'calories_per_serving': 0.0,\n",
    "        'protein_content': 0.0,\n",
    "        'fiber_content': 0.0\n",
    "    }\n",
    "    \n",
    "    if pd.isna(catalog_text):\n",
    "        return features\n",
    "    \n",
    "    text = str(catalog_text).lower()\n",
    "    \n",
    "    # Extract item name\n",
    "    item_name_match = re.search(r'item name:\\s*([^\\n]+)', catalog_text, re.IGNORECASE)\n",
    "    if item_name_match:\n",
    "        features['item_name'] = item_name_match.group(1).strip()\n",
    "    \n",
    "    # Extract pack size information\n",
    "    pack_patterns = [\n",
    "        r'pack of\\s*(\\d+)',\n",
    "        r'(\\d+)\\s*count',\n",
    "        r'(\\d+)\\s*pack',\n",
    "        r'(\\d+)\\s*ct'\n",
    "    ]\n",
    "    for pattern in pack_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            features['pack_size'] = match.group(1)\n",
    "            break\n",
    "    \n",
    "    # Extract weight\n",
    "    weight_patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*oz',\n",
    "        r'(\\d+\\.?\\d*)\\s*ounce',\n",
    "        r'(\\d+\\.?\\d*)\\s*lb',\n",
    "        r'(\\d+\\.?\\d*)\\s*pound'\n",
    "    ]\n",
    "    for pattern in weight_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                features['weight'] = float(match.group(1))\n",
    "            except:\n",
    "                features['weight'] = 0.0\n",
    "            break\n",
    "    \n",
    "    # Extract volume\n",
    "    volume_patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*fl\\s*oz',\n",
    "        r'(\\d+\\.?\\d*)\\s*fluid\\s*ounce',\n",
    "        r'(\\d+\\.?\\d*)\\s*ml',\n",
    "        r'(\\d+\\.?\\d*)\\s*liter'\n",
    "    ]\n",
    "    for pattern in volume_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                features['volume'] = float(match.group(1))\n",
    "            except:\n",
    "                features['volume'] = 0.0\n",
    "            break\n",
    "    \n",
    "    # Extract count\n",
    "    count_patterns = [\n",
    "        r'(\\d+)\\s*tea bags',\n",
    "        r'(\\d+)\\s*capsules',\n",
    "        r'(\\d+)\\s*pods',\n",
    "        r'(\\d+)\\s*cookies'\n",
    "    ]\n",
    "    for pattern in count_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                features['count'] = float(match.group(1))\n",
    "            except:\n",
    "                features['count'] = 0.0\n",
    "            break\n",
    "    \n",
    "    # Extract unit from the structured Unit field\n",
    "    unit_match = re.search(r'unit:\\s*([^\\n]+)', catalog_text, re.IGNORECASE)\n",
    "    if unit_match:\n",
    "        features['unit'] = unit_match.group(1).strip().lower()\n",
    "    \n",
    "    # Extract flavor information\n",
    "    flavor_keywords = ['vanilla', 'chocolate', 'strawberry', 'lemon', 'mint', 'berry', \n",
    "                      'caramel', 'honey', 'spice', 'cinnamon', 'ginger', 'peach']\n",
    "    for flavor in flavor_keywords:\n",
    "        if flavor in text:\n",
    "            features['flavor'] = flavor\n",
    "            break\n",
    "    \n",
    "    # Extract brand names\n",
    "    brand_patterns = [\n",
    "        r'manufacturer:\\s*([^\\n]+)',\n",
    "        r'brand:\\s*([^\\n]+)'\n",
    "    ]\n",
    "    for pattern in brand_patterns:\n",
    "        match = re.search(pattern, catalog_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            features['brand'] = match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # Health and dietary attributes\n",
    "    features['is_organic'] = 1 if any(term in text for term in ['organic', 'usda organic']) else 0\n",
    "    features['is_gluten_free'] = 1 if 'gluten free' in text else 0\n",
    "    features['is_vegan'] = 1 if 'vegan' in text else 0\n",
    "    features['is_kosher'] = 1 if 'kosher' in text else 0\n",
    "    features['is_sugar_free'] = 1 if any(term in text for term in ['sugar free', 'no sugar', 'zero sugar']) else 0\n",
    "    features['is_low_carb'] = 1 if any(term in text for term in ['low carb', 'keto', 'keto-friendly']) else 0\n",
    "    features['is_non_gmo'] = 1 if any(term in text for term in ['non-gmo', 'non gmo']) else 0\n",
    "    \n",
    "    # Extract nutritional information\n",
    "    calorie_match = re.search(r'(\\d+)\\s*calories', text)\n",
    "    if calorie_match:\n",
    "        try:\n",
    "            features['calories_per_serving'] = float(calorie_match.group(1))\n",
    "        except:\n",
    "            features['calories_per_serving'] = 0.0\n",
    "    \n",
    "    protein_match = re.search(r'(\\d+\\.?\\d*)\\s*g\\s*protein', text)\n",
    "    if protein_match:\n",
    "        try:\n",
    "            features['protein_content'] = float(protein_match.group(1))\n",
    "        except:\n",
    "            features['protein_content'] = 0.0\n",
    "    \n",
    "    fiber_match = re.search(r'(\\d+\\.?\\d*)\\s*g\\s*fiber', text)\n",
    "    if fiber_match:\n",
    "        try:\n",
    "            features['fiber_content'] = float(fiber_match.group(1))\n",
    "        except:\n",
    "            features['fiber_content'] = 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_derived_features(df, has_price=True):\n",
    "    \"\"\"\n",
    "    Create derived features, handling the case where price might be missing\n",
    "    \"\"\"\n",
    "    # Product category based on item name\n",
    "    def categorize_product(item_name):\n",
    "        if pd.isna(item_name):\n",
    "            return 'other'\n",
    "        item_name = str(item_name).lower()\n",
    "        if any(word in item_name for word in ['tea', 'chai']):\n",
    "            return 'tea'\n",
    "        elif any(word in item_name for word in ['coffee', 'brew']):\n",
    "            return 'coffee'\n",
    "        elif any(word in item_name for word in ['snack', 'chip', 'cracker', 'cookie']):\n",
    "            return 'snack'\n",
    "        elif any(word in item_name for word in ['sauce', 'dressing', 'oil']):\n",
    "            return 'condiment'\n",
    "        elif any(word in item_name for word in ['spice', 'seasoning']):\n",
    "            return 'spice'\n",
    "        elif any(word in item_name for word in ['candy', 'chocolate']):\n",
    "            return 'candy'\n",
    "        elif any(word in item_name for word in ['pasta', 'rice', 'grain']):\n",
    "            return 'grain'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    df['product_category'] = df['item_name'].apply(categorize_product)\n",
    "    \n",
    "    # Text length features\n",
    "    df['catalog_content_length'] = df['catalog_content'].str.len().fillna(0)\n",
    "    \n",
    "    # Only create price-related features if price is available\n",
    "    if has_price and 'price' in df.columns:\n",
    "        if 'weight' in df.columns:\n",
    "            weight_safe = df['weight'].replace(0, np.nan)\n",
    "            df['price_per_oz'] = df['price'] / weight_safe\n",
    "            df['price_per_oz'] = df['price_per_oz'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        if 'volume' in df.columns:\n",
    "            volume_safe = df['volume'].replace(0, np.nan)\n",
    "            df['price_per_fl_oz'] = df['price'] / volume_safe\n",
    "            df['price_per_fl_oz'] = df['price_per_fl_oz'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        if 'count' in df.columns:\n",
    "            count_safe = df['count'].replace(0, np.nan)\n",
    "            df['price_per_count'] = df['price'] / count_safe\n",
    "            df['price_per_count'] = df['price_per_count'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "class RobustSMAPEStackingPricePredictor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.feature_columns = []\n",
    "        self.expected_columns = []\n",
    "        self.stacking_model = None\n",
    "        self.is_trained = False\n",
    "        self.best_params_ = None\n",
    "        \n",
    "    def preprocess_data(self, df, is_training=True):\n",
    "        \"\"\"\n",
    "        Preprocess the data: extract features, handle missing values, encode categorical variables\n",
    "        \"\"\"\n",
    "        print(\"Step 1: Extracting features from catalog content...\")\n",
    "        \n",
    "        # Make a copy to avoid modifying original data\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Extract features\n",
    "        catalog_features = df_processed['catalog_content'].apply(extract_features_from_catalog)\n",
    "        features_df = pd.DataFrame(catalog_features.tolist(), index=df_processed.index)\n",
    "        \n",
    "        # Concatenate with original data\n",
    "        df_enhanced = pd.concat([df_processed, features_df], axis=1)\n",
    "        \n",
    "        # Ensure all expected numerical columns exist with proper defaults\n",
    "        numerical_columns = ['weight', 'volume', 'count', 'calories_per_serving', 'protein_content', 'fiber_content']\n",
    "        for col in numerical_columns:\n",
    "            if col not in df_enhanced.columns:\n",
    "                df_enhanced[col] = 0.0\n",
    "            else:\n",
    "                df_enhanced[col] = pd.to_numeric(df_enhanced[col], errors='coerce').fillna(0.0)\n",
    "        \n",
    "        # Ensure all expected boolean columns exist\n",
    "        boolean_columns = ['is_organic', 'is_gluten_free', 'is_vegan', 'is_kosher', \n",
    "                          'is_sugar_free', 'is_low_carb', 'is_non_gmo']\n",
    "        for col in boolean_columns:\n",
    "            if col not in df_enhanced.columns:\n",
    "                df_enhanced[col] = 0\n",
    "            else:\n",
    "                df_enhanced[col] = df_enhanced[col].astype(int)\n",
    "        \n",
    "        # Handle categorical columns\n",
    "        categorical_columns = ['unit', 'flavor', 'brand', 'item_name']\n",
    "        for col in categorical_columns:\n",
    "            if col not in df_enhanced.columns:\n",
    "                df_enhanced[col] = 'unknown'\n",
    "            else:\n",
    "                df_enhanced[col] = df_enhanced[col].fillna('unknown')\n",
    "        \n",
    "        # Create derived features\n",
    "        has_price = is_training and 'price' in df_enhanced.columns\n",
    "        df_enhanced = create_derived_features(df_enhanced, has_price=has_price)\n",
    "        \n",
    "        # Handle outliers in price if it exists and we're training\n",
    "        if is_training and 'price' in df_enhanced.columns:\n",
    "            Q1 = df_enhanced['price'].quantile(0.25)\n",
    "            Q3 = df_enhanced['price'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = max(0, Q1 - 1.5 * IQR)  # Ensure lower bound is not negative\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            df_enhanced['price'] = np.where(df_enhanced['price'] > upper_bound, upper_bound, df_enhanced['price'])\n",
    "            df_enhanced['price'] = np.where(df_enhanced['price'] < lower_bound, lower_bound, df_enhanced['price'])\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    def prepare_features(self, df, is_training=True):\n",
    "        \"\"\"\n",
    "        Prepare features for modeling with robust error handling\n",
    "        \"\"\"\n",
    "        # Define core features that should always be present\n",
    "        core_features = [\n",
    "            'weight', 'volume', 'count', 'catalog_content_length',\n",
    "            'is_organic', 'is_gluten_free', 'is_vegan', 'is_kosher', \n",
    "            'is_sugar_free', 'is_low_carb', 'is_non_gmo'\n",
    "        ]\n",
    "        \n",
    "        # Add price per unit features if available (only in training)\n",
    "        if is_training:\n",
    "            price_per_features = [col for col in df.columns if col.startswith('price_per')]\n",
    "            feature_columns = core_features + price_per_features\n",
    "        else:\n",
    "            feature_columns = core_features.copy()\n",
    "        \n",
    "        # Ensure all feature columns exist in the dataframe\n",
    "        missing_features = set(feature_columns) - set(df.columns)\n",
    "        for feature in missing_features:\n",
    "            df[feature] = 0.0  # Add missing features with default value\n",
    "        \n",
    "        # Select only the feature columns we want\n",
    "        feature_columns = [col for col in feature_columns if col in df.columns]\n",
    "        X = df[feature_columns].copy()\n",
    "        \n",
    "        # Replace infinite values with NaN\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        if is_training:\n",
    "            self.feature_columns = feature_columns\n",
    "            self.expected_columns = feature_columns.copy()\n",
    "            \n",
    "            # Fit imputer and scaler on training data\n",
    "            X_imputed = self.imputer.fit_transform(X)\n",
    "            \n",
    "            # Scale numerical features\n",
    "            numerical_cols = [col for col in X.columns if X[col].dtype in ['float64', 'float32', 'int64']]\n",
    "            if numerical_cols:\n",
    "                X_imputed = self.scaler.fit_transform(X_imputed)\n",
    "            \n",
    "            X_processed = pd.DataFrame(X_imputed, columns=feature_columns, index=X.index)\n",
    "            \n",
    "            if 'price' in df.columns:\n",
    "                y = df['price'].copy()\n",
    "                return X_processed, y\n",
    "            else:\n",
    "                return X_processed\n",
    "        else:\n",
    "            # Ensure we have the same columns as training\n",
    "            missing_cols = set(self.expected_columns) - set(X.columns)\n",
    "            extra_cols = set(X.columns) - set(self.expected_columns)\n",
    "            \n",
    "            for col in missing_cols:\n",
    "                X[col] = 0.0\n",
    "            for col in extra_cols:\n",
    "                X = X.drop(col, axis=1)\n",
    "            \n",
    "            # Reorder columns to match training\n",
    "            X = X[self.expected_columns]\n",
    "            \n",
    "            # Transform using fitted imputer and scaler\n",
    "            X_imputed = self.imputer.transform(X)\n",
    "            X_imputed = self.scaler.transform(X_imputed)\n",
    "            \n",
    "            X_processed = pd.DataFrame(X_imputed, columns=self.expected_columns, index=X.index)\n",
    "            return X_processed\n",
    "    \n",
    "    def build_stacking_regressor(self):\n",
    "        \"\"\"\n",
    "        Build a StackingRegressor with multiple base models\n",
    "        \"\"\"\n",
    "        # Base models with robust parameters\n",
    "        base_models = [\n",
    "            ('ridge', Ridge(alpha=1.0, random_state=42)),\n",
    "            ('lasso', Lasso(alpha=0.1, random_state=42)),\n",
    "            ('rf', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "            ('gbm', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "            ('svr', SVR(kernel='rbf', C=1.0))\n",
    "        ]\n",
    "        \n",
    "        # Meta-model\n",
    "        meta_model = LinearRegression()\n",
    "        \n",
    "        # Create StackingRegressor\n",
    "        stacking_regressor = StackingRegressor(\n",
    "            estimators=base_models,\n",
    "            final_estimator=meta_model,\n",
    "            cv=5,\n",
    "            passthrough=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return stacking_regressor\n",
    "    \n",
    "    def _calculate_aggregated_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Calculate feature importance by aggregating importances from all base models\n",
    "        \"\"\"\n",
    "        # Initialize importance array\n",
    "        aggregated_importance = np.zeros(len(self.feature_columns))\n",
    "        \n",
    "        # Get importances from each base model\n",
    "        for name, model in self.stacking_model.estimators_:\n",
    "            try:\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    # Tree-based models (RandomForest, GradientBoosting)\n",
    "                    importance = model.feature_importances_\n",
    "                elif hasattr(model, 'coef_'):\n",
    "                    # Linear models (Ridge, Lasso)\n",
    "                    importance = abs(model.coef_)\n",
    "                else:\n",
    "                    # Models without feature importance (SVR) - skip\n",
    "                    continue\n",
    "                \n",
    "                # Ensure the importance array matches our feature dimensions\n",
    "                if len(importance) == len(self.feature_columns):\n",
    "                    aggregated_importance += importance\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not get importance from {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize the importance scores\n",
    "        if aggregated_importance.sum() > 0:\n",
    "            aggregated_importance = aggregated_importance / aggregated_importance.sum()\n",
    "        \n",
    "        # Create DataFrame\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': self.feature_columns,\n",
    "            'importance': aggregated_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return feature_importance_df\n",
    "    \n",
    "    def train(self, train_df, perform_tuning=True):\n",
    "        print(\"Starting SMAPE-tuned StackingRegressor training...\")\n",
    "\n",
    "        # Preprocess training data\n",
    "        df_processed = self.preprocess_data(train_df, is_training=True)\n",
    "\n",
    "        # Prepare features\n",
    "        X, y = self.prepare_features(df_processed, is_training=True)\n",
    "\n",
    "        print(f\"Training data shape: {X.shape}\")\n",
    "        print(f\"Target variable shape: {y.shape}\")\n",
    "        print(f\"Features used: {len(self.feature_columns)}\")\n",
    "\n",
    "        # Split data for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Build stacking regressor\n",
    "        stacking_model = self.build_stacking_regressor()\n",
    "\n",
    "        if perform_tuning:\n",
    "            print(\"Performing randomized hyperparameter tuning with SMAPE scoring...\")\n",
    "\n",
    "            # Define hyperparameter search space\n",
    "            param_distributions = {\n",
    "                'ridge__alpha': np.logspace(-2, 2, 10),\n",
    "                'lasso__alpha': np.logspace(-3, 1, 10),\n",
    "                'rf__n_estimators': np.arange(50, 300, 50),\n",
    "                'rf__max_depth': [None, 5, 10, 15, 20],\n",
    "                'gbm__n_estimators': np.arange(50, 300, 50),\n",
    "                'gbm__learning_rate': np.linspace(0.01, 0.2, 10),\n",
    "                'svr__C': np.logspace(-2, 2, 10),\n",
    "                'svr__gamma': ['scale', 'auto']\n",
    "            }\n",
    "\n",
    "            # Perform RandomizedSearchCV with SMAPE\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=stacking_model,\n",
    "                param_distributions=param_distributions,\n",
    "                n_iter=20,\n",
    "                scoring=smape_scorer,\n",
    "                cv=3,\n",
    "                verbose=2,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            random_search.fit(X_train, y_train)\n",
    "\n",
    "            # Best model and parameters\n",
    "            self.stacking_model = random_search.best_estimator_\n",
    "            self.best_params_ = random_search.best_params_\n",
    "\n",
    "            print(f\"Best parameters found: {self.best_params_}\")\n",
    "\n",
    "        else:\n",
    "            print(\"Training without hyperparameter tuning...\")\n",
    "            self.stacking_model = stacking_model\n",
    "            self.stacking_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model on validation set\n",
    "        y_pred = self.stacking_model.predict(X_val)\n",
    "\n",
    "        val_smape = smape(y_val, y_pred)\n",
    "        val_mae = mean_absolute_error(y_val, y_pred)\n",
    "        val_mse = mean_squared_error(y_val, y_pred)\n",
    "        val_r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "        print(\"\\n=== VALIDATION PERFORMANCE ===\")\n",
    "        print(f\"SMAPE: {val_smape:.4f}%\")\n",
    "        print(f\"MAE: ${val_mae:.2f}\")\n",
    "        print(f\"MSE: ${val_mse:.2f}\")\n",
    "        print(f\"R²: {val_r2:.4f}\")\n",
    "\n",
    "        # Cross-validation SMAPE\n",
    "        cv_scores = cross_val_score(\n",
    "            self.stacking_model, X, y, cv=3, scoring=smape_scorer, n_jobs=-1\n",
    "        )\n",
    "        cv_smape_scores = -cv_scores  # Convert back to positive values\n",
    "\n",
    "        print(f\"\\nCross-validation SMAPE: {cv_smape_scores.mean():.4f}% (+/- {cv_smape_scores.std() * 2:.4f}%)\")\n",
    "\n",
    "        # Retrain on full dataset\n",
    "        print(\"\\nRetraining on full training dataset...\")\n",
    "        self.stacking_model.fit(X, y)\n",
    "        self.is_trained = True\n",
    "\n",
    "        # FIXED: Proper feature importance for stacking regressor\n",
    "        print(\"\\n=== STACKING MODEL ANALYSIS ===\")\n",
    "        \n",
    "        # Show base model weights in the meta-model\n",
    "        if hasattr(self.stacking_model.final_estimator_, 'coef_'):\n",
    "            base_model_names = [name for name, _ in self.stacking_model.estimators_]\n",
    "            meta_coefficients = abs(self.stacking_model.final_estimator_.coef_)\n",
    "            \n",
    "            # Create a DataFrame showing base model importance in the meta-model\n",
    "            meta_importance = pd.DataFrame({\n",
    "                'base_model': base_model_names,\n",
    "                'meta_weight': meta_coefficients\n",
    "            }).sort_values('meta_weight', ascending=False)\n",
    "            \n",
    "            print(\"Base Model Weights in Meta-Model:\")\n",
    "            print(meta_importance)\n",
    "        \n",
    "        # Calculate aggregated feature importance from base models\n",
    "        try:\n",
    "            feature_importance = self._calculate_aggregated_feature_importance()\n",
    "            print(\"\\nTop 10 Most Important Features (Aggregated from Base Models):\")\n",
    "            print(feature_importance.head(10))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nNote: Feature importance calculation skipped: {e}\")\n",
    "\n",
    "        return {\n",
    "            'smape': val_smape,\n",
    "            'mae': val_mae,\n",
    "            'mse': val_mse,\n",
    "            'r2': val_r2,\n",
    "            'cv_smape_mean': cv_smape_scores.mean(),\n",
    "            'cv_smape_std': cv_smape_scores.std()\n",
    "        }\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        \"\"\"\n",
    "        Predict prices for test data using the trained StackingRegressor\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        print(\"Making predictions on test data...\")\n",
    "        \n",
    "        # Preprocess test data\n",
    "        df_processed = self.preprocess_data(test_df, is_training=False)\n",
    "        \n",
    "        # Prepare features\n",
    "        X_test = self.prepare_features(df_processed, is_training=False)\n",
    "        \n",
    "        print(f\"Test data shape after preprocessing: {X_test.shape}\")\n",
    "        print(f\"Expected features: {len(self.expected_columns)}\")\n",
    "        print(f\"Actual features: {len(X_test.columns)}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.stacking_model.predict(X_test)\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results = test_df.copy()\n",
    "        results['predicted_price'] = predictions\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model and preprocessors\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"No trained model to save\")\n",
    "        \n",
    "        model_data = {\n",
    "            'scaler': self.scaler,\n",
    "            'imputer': self.imputer,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'expected_columns': self.expected_columns,\n",
    "            'stacking_model': self.stacking_model,\n",
    "            'best_params': self.best_params_,\n",
    "            'is_trained': self.is_trained\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"SMAPE-tuned StackingRegressor saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Load a trained model and preprocessors\n",
    "        \"\"\"\n",
    "        model_data = joblib.load(filepath)\n",
    "        \n",
    "        self.scaler = model_data['scaler']\n",
    "        self.imputer = model_data['imputer']\n",
    "        self.feature_columns = model_data['feature_columns']\n",
    "        self.expected_columns = model_data['expected_columns']\n",
    "        self.stacking_model = model_data['stacking_model']\n",
    "        self.best_params_ = model_data['best_params']\n",
    "        self.is_trained = model_data['is_trained']\n",
    "        \n",
    "        print(f\"SMAPE-tuned StackingRegressor loaded from {filepath}\")\n",
    "        if self.best_params_:\n",
    "            print(f\"Best parameters: {self.best_params_}\")\n",
    "\n",
    "# Usage example:\n",
    "def run_complete_pipeline(train_df, test_df, perform_tuning=True):\n",
    "    \"\"\"\n",
    "    Complete pipeline for training and prediction\n",
    "    \"\"\"\n",
    "    print(\"=== AMAZON PRODUCT PRICE PREDICTION PIPELINE ===\")\n",
    "    \n",
    "    # Initialize predictor\n",
    "    smape_predictor = RobustSMAPEStackingPricePredictor()\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        training_results = smape_predictor.train(train_df, perform_tuning=perform_tuning)\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions = smape_predictor.predict(test_df)\n",
    "        \n",
    "        print(\"\\n=== PREDICTION COMPLETE ===\")\n",
    "        print(f\"Predicted prices for {len(test_predictions)} test samples\")\n",
    "        print(f\"Price range: ${test_predictions['predicted_price'].min():.2f} - ${test_predictions['predicted_price'].max():.2f}\")\n",
    "        \n",
    "        return smape_predictor, training_results, test_predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in pipeline: {e}\")\n",
    "        import traceback\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05cb8982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AMAZON PRODUCT PRICE PREDICTION PIPELINE ===\n",
      "\n",
      "1. Loading training data...\n",
      "Training data shape: (50000, 4)\n",
      "Training columns: ['sample_id', 'catalog_content', 'image_link', 'price']\n",
      "Available prices in training data: 50000\n",
      "Missing prices in training data: 0\n",
      "\n",
      "2. Loading test data...\n",
      "Test data shape: (25000, 3)\n",
      "Test columns: ['sample_id', 'catalog_content', 'image_link']\n",
      "\n",
      "3. Training SMAPE-tuned StackingRegressor...\n",
      "Starting SMAPE-tuned StackingRegressor training...\n",
      "Step 1: Extracting features from catalog content...\n",
      "Training data shape: (50000, 14)\n",
      "Target variable shape: (50000,)\n",
      "Features used: 14\n",
      "Performing randomized hyperparameter tuning with SMAPE scoring...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best parameters found: {'svr__gamma': 'auto', 'svr__C': np.float64(0.0774263682681127), 'ridge__alpha': np.float64(35.93813663804626), 'rf__n_estimators': np.int64(100), 'rf__max_depth': 20, 'lasso__alpha': np.float64(0.1668100537200059), 'gbm__n_estimators': np.int64(100), 'gbm__learning_rate': np.float64(0.03111111111111111)}\n",
      "\n",
      "=== VALIDATION PERFORMANCE ===\n",
      "SMAPE: 16.2584%\n",
      "MAE: $3.08\n",
      "MSE: $59.42\n",
      "R²: 0.8064\n",
      "\n",
      "Cross-validation SMAPE: 17.0972% (+/- 0.3713%)\n",
      "\n",
      "Retraining on full training dataset...\n",
      "Error in pipeline: All arrays must be of the same length\n",
      "Traceback: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SHRAMAN\\AppData\\Local\\Temp\\ipykernel_15944\\324691388.py\", line 43, in run_complete_pipeline\n",
      "    training_results = smape_predictor.train(train_df, perform_tuning=True)\n",
      "  File \"C:\\Users\\SHRAMAN\\AppData\\Local\\Temp\\ipykernel_15944\\2553421792.py\", line 255, in train\n",
      "    feature_importance = pd.DataFrame({\n",
      "                         ~~~~~~~~~~~~^^\n",
      "       'feature': self.feature_columns,\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "       'importance': abs(self.stacking_model.final_estimator_.coef_)\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    }).sort_values('importance', ascending=False)\n",
      "    ^^\n",
      "  File \"d:\\Play Around Folder\\PlayStore-DataSet-ML\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 782, in __init__\n",
      "    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n",
      "  File \"d:\\Play Around Folder\\PlayStore-DataSet-ML\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 503, in dict_to_mgr\n",
      "    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n",
      "  File \"d:\\Play Around Folder\\PlayStore-DataSet-ML\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 114, in arrays_to_mgr\n",
      "    index = _extract_index(arrays)\n",
      "  File \"d:\\Play Around Folder\\PlayStore-DataSet-ML\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 677, in _extract_index\n",
      "    raise ValueError(\"All arrays must be of the same length\")\n",
      "ValueError: All arrays must be of the same length\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MAIN EXECUTION WITH ERROR HANDLING\n",
    "\n",
    "def run_complete_pipeline(train_file='../data/train.csv', test_file='../data/test.csv'):\n",
    "    \"\"\"\n",
    "    Complete pipeline with robust error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"=== AMAZON PRODUCT PRICE PREDICTION PIPELINE ===\\n\")\n",
    "        \n",
    "        # Step 1: Load training data (with prices)\n",
    "        print(\"1. Loading training data...\")\n",
    "        train_df = pd.read_csv(train_file)\n",
    "        \n",
    "        print(f\"Training data shape: {train_df.shape}\")\n",
    "        print(\"Training columns:\", train_df.columns.tolist())\n",
    "        \n",
    "        if 'price' not in train_df.columns:\n",
    "            raise ValueError(\"Training data must contain 'price' column\")\n",
    "        \n",
    "        missing_prices = train_df['price'].isnull().sum()\n",
    "        print(f\"Available prices in training data: {len(train_df) - missing_prices}\")\n",
    "        print(f\"Missing prices in training data: {missing_prices}\")\n",
    "        \n",
    "        # Step 2: Load test data (without prices)\n",
    "        print(\"\\n2. Loading test data...\")\n",
    "        test_df = pd.read_csv(test_file)\n",
    "        \n",
    "        print(f\"Test data shape: {test_df.shape}\")\n",
    "        print(\"Test columns:\", test_df.columns.tolist())\n",
    "        \n",
    "        if 'price' in test_df.columns:\n",
    "            if test_df['price'].isnull().all():\n",
    "                print(\"Test data contains 'price' column with all null values - will ignore for predictions\")\n",
    "                test_df = test_df.drop('price', axis=1)\n",
    "            else:\n",
    "                print(\"Warning: Test data contains non-null 'price' column\")\n",
    "        \n",
    "        # Step 3: Initialize and train the model\n",
    "        print(\"\\n3. Training SMAPE-tuned StackingRegressor...\")\n",
    "        smape_predictor = RobustSMAPEStackingPricePredictor()\n",
    "        \n",
    "        # Train with hyperparameter tuning\n",
    "        training_results = smape_predictor.train(train_df, perform_tuning=True)\n",
    "        \n",
    "        # Step 4: Save the trained model\n",
    "        print(\"\\n4. Saving trained model...\")\n",
    "        smape_predictor.save_model('../Model/robust_smape_stacking_predictor.pkl')\n",
    "        \n",
    "        # Step 5: Make predictions on test data\n",
    "        print(\"\\n5. Making predictions on test data...\")\n",
    "        test_predictions = smape_predictor.predict(test_df)\n",
    "        \n",
    "        print(\"Prediction completed!\")\n",
    "        print(f\"Predicted prices for {len(test_predictions)} test products\")\n",
    "        \n",
    "        # Step 6: Display prediction results\n",
    "        print(\"\\n6. Prediction results:\")\n",
    "        print(test_predictions[['sample_id', 'predicted_price']].head(10))\n",
    "        \n",
    "        print(\"\\nTest Prediction Statistics:\")\n",
    "        print(test_predictions['predicted_price'].describe())\n",
    "        \n",
    "        # Step 7: Save results\n",
    "        print(\"\\n7. Saving results...\")\n",
    "        test_predictions[['sample_id', 'predicted_price']].to_csv('../Model/final_test_predictions.csv', index=False)\n",
    "        print(\"Predictions saved to 'final_test_predictions.csv'\")\n",
    "        \n",
    "        return smape_predictor, test_predictions, training_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in pipeline: {str(e)}\")\n",
    "        print(\"Traceback:\", traceback.format_exc())\n",
    "        return None, None, None\n",
    "\n",
    "# Run the complete pipeline\n",
    "predictor, predictions, results = run_complete_pipeline('../data/train.csv', '../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41622028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION AND ANALYSIS\n",
    "\n",
    "if predictions is not None:\n",
    "    # Enhanced visualization for test predictions\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    # 1. Distribution of predicted prices in test set\n",
    "    plt.subplot(1, 4, 1)\n",
    "    sns.histplot(predictions['predicted_price'], bins=50, kde=True)\n",
    "    plt.title('Distribution of Predicted Prices\\n(Test Set)')\n",
    "    plt.xlabel('Predicted Price ($)')\n",
    "    \n",
    "    # 2. Price by product category in test set\n",
    "    plt.subplot(1, 4, 2)\n",
    "    if 'product_category' in predictions.columns:\n",
    "        category_prices = predictions.groupby('product_category')['predicted_price'].mean().sort_values(ascending=False)\n",
    "        sns.barplot(y=category_prices.index, x=category_prices.values)\n",
    "        plt.title('Average Predicted Price by Category\\n(Test Set)')\n",
    "        plt.xlabel('Average Price ($)')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Product Category\\nNot Available', \n",
    "                horizontalalignment='center', verticalalignment='center',\n",
    "                transform=plt.gca().transAxes)\n",
    "        plt.title('Product Category Analysis')\n",
    "    \n",
    "    # 3. Feature importance\n",
    "    plt.subplot(1, 4, 3)\n",
    "    if hasattr(predictor.stacking_model.final_estimator_, 'coef_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': predictor.feature_columns,\n",
    "            'importance': abs(predictor.stacking_model.final_estimator_.coef_)\n",
    "        }).sort_values('importance', ascending=False).head(10)\n",
    "        \n",
    "        sns.barplot(data=feature_importance, y='feature', x='importance')\n",
    "        plt.title('Top 10 Feature Importances')\n",
    "        plt.xlabel('Importance')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Feature Importance\\nNot Available', \n",
    "                horizontalalignment='center', verticalalignment='center',\n",
    "                transform=plt.gca().transAxes)\n",
    "        plt.title('Feature Importance')\n",
    "    \n",
    "    # 4. SMAPE performance\n",
    "    plt.subplot(1, 4, 4)\n",
    "    metrics = ['SMAPE', 'MAE', 'R²']\n",
    "    values = [results['smape'], results['mae'], results['r2']]\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    normalized_values = [results['smape']/100, results['mae']/max(values[1], 1), results['r2']]\n",
    "    \n",
    "    bars = plt.bar(metrics, normalized_values)\n",
    "    plt.title('Model Performance Metrics\\n(Normalized)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (metric, value) in enumerate(zip(metrics, values)):\n",
    "        if metric == 'SMAPE':\n",
    "            plt.text(i, normalized_values[i] + 0.02, f'{value:.2f}%', ha='center')\n",
    "        elif metric == 'MAE':\n",
    "            plt.text(i, normalized_values[i] + 0.02, f'${value:.2f}', ha='center')\n",
    "        else:\n",
    "            plt.text(i, normalized_values[i] + 0.02, f'{value:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\n=== DETAILED RESULTS ===\")\n",
    "    print(f\"Best Model: {type(predictor.stacking_model).__name__}\")\n",
    "    print(f\"Validation SMAPE: {results['smape']:.4f}%\")\n",
    "    print(f\"Validation MAE: ${results['mae']:.2f}\")\n",
    "    print(f\"Validation R²: {results['r2']:.4f}\")\n",
    "    print(f\"Cross-validation SMAPE: {results['cv_smape_mean']:.4f}% ± {results['cv_smape_std']:.4f}%\")\n",
    "    \n",
    "    if predictor.best_params_:\n",
    "        print(f\"\\nBest Hyperparameters: {predictor.best_params_}\")\n",
    "    \n",
    "    print(f\"\\nFeatures used: {len(predictor.feature_columns)}\")\n",
    "    print(\"All features:\", predictor.feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "508240f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AMAZON PRODUCT PRICE PREDICTION PIPELINE ===\n",
      "\n",
      "1. Loading training data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 719\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;66;03m# Run the complete pipeline\u001b[39;00m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     predictor, predictions, results = \u001b[43mrun_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/train.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/test.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m predictor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== PIPELINE COMPLETED SUCCESSFULLY ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 648\u001b[39m, in \u001b[36mrun_complete_pipeline\u001b[39m\u001b[34m(train_file, test_file)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;66;03m# Step 1: Load training data (with prices)\u001b[39;00m\n\u001b[32m    647\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m1. Loading training data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    651\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining columns:\u001b[39m\u001b[33m\"\u001b[39m, train_df.columns.tolist())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Play Around Folder\\PlayStore-DataSet-ML\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Play Around Folder\\PlayStore-DataSet-ML\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Play Around Folder\\PlayStore-DataSet-ML\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Play Around Folder\\PlayStore-DataSet-ML\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:236\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    234\u001b[39m     chunks = \u001b[38;5;28mself\u001b[39m._reader.read_low_memory(nrows)\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     data = \u001b[43m_concatenate_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    239\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._reader.read(nrows)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Play Around Folder\\PlayStore-DataSet-ML\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:376\u001b[39m, in \u001b[36m_concatenate_chunks\u001b[39m\u001b[34m(chunks)\u001b[39m\n\u001b[32m    374\u001b[39m     result[name] = union_categoricals(arrs, sort_categories=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result[name] = \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(non_cat_dtypes) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m result[name].dtype == np.dtype(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m    378\u001b[39m         warning_columns.append(\u001b[38;5;28mstr\u001b[39m(name))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Play Around Folder\\PlayStore-DataSet-ML\\.venv\\Lib\\site-packages\\pandas\\core\\dtypes\\concat.py:78\u001b[39m, in \u001b[36mconcat_compat\u001b[39m\u001b[34m(to_concat, axis, ea_compat_axis)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np.ndarray):\n\u001b[32m     77\u001b[39m     to_concat_arrs = cast(\u001b[33m\"\u001b[39m\u001b[33mSequence[np.ndarray]\u001b[39m\u001b[33m\"\u001b[39m, to_concat)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat_arrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m to_concat_eas = cast(\u001b[33m\"\u001b[39m\u001b[33mSequence[ExtensionArray]\u001b[39m\u001b[33m\"\u001b[39m, to_concat)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ea_compat_axis:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# We have 1D objects, that don't support axis keyword\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "import joblib\n",
    "\n",
    "# ========== UTILITY FUNCTIONS ==========\n",
    "\n",
    "def smape(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    \"\"\"\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Handle cases where both actual and predicted are zero\n",
    "    denominator = (np.abs(actual) + np.abs(predicted))\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    denominator = np.where(denominator == 0, 1e-10, denominator)\n",
    "    \n",
    "    smape_value = (100 / len(actual)) * np.sum(2 * np.abs(predicted - actual) / denominator)\n",
    "    return smape_value\n",
    "\n",
    "# Create SMAPE scorer for sklearn (lower is better)\n",
    "smape_scorer = make_scorer(\n",
    "    lambda y_true, y_pred: smape(y_true, y_pred),\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "def extract_features_from_catalog(catalog_text):\n",
    "    \"\"\"\n",
    "    Extract structured features from the catalog_content field\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'item_name': '',\n",
    "        'pack_size': '',\n",
    "        'weight': 0.0,\n",
    "        'volume': 0.0,\n",
    "        'count': 0.0,\n",
    "        'unit': '',\n",
    "        'flavor': '',\n",
    "        'brand': '',\n",
    "        'is_organic': 0,\n",
    "        'is_gluten_free': 0,\n",
    "        'is_vegan': 0,\n",
    "        'is_kosher': 0,\n",
    "        'is_sugar_free': 0,\n",
    "        'is_low_carb': 0,\n",
    "        'is_non_gmo': 0,\n",
    "        'calories_per_serving': 0.0,\n",
    "        'protein_content': 0.0,\n",
    "        'fiber_content': 0.0\n",
    "    }\n",
    "    \n",
    "    if pd.isna(catalog_text):\n",
    "        return features\n",
    "    \n",
    "    text = str(catalog_text).lower()\n",
    "    \n",
    "    # Extract item name\n",
    "    item_name_match = re.search(r'item name:\\s*([^\\n]+)', catalog_text, re.IGNORECASE)\n",
    "    if item_name_match:\n",
    "        features['item_name'] = item_name_match.group(1).strip()\n",
    "    \n",
    "    # Extract pack size information\n",
    "    pack_patterns = [\n",
    "        r'pack of\\s*(\\d+)',\n",
    "        r'(\\d+)\\s*count',\n",
    "        r'(\\d+)\\s*pack',\n",
    "        r'(\\d+)\\s*ct'\n",
    "    ]\n",
    "    for pattern in pack_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            features['pack_size'] = match.group(1)\n",
    "            break\n",
    "    \n",
    "    # Extract weight\n",
    "    weight_patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*oz',\n",
    "        r'(\\d+\\.?\\d*)\\s*ounce',\n",
    "        r'(\\d+\\.?\\d*)\\s*lb',\n",
    "        r'(\\d+\\.?\\d*)\\s*pound'\n",
    "    ]\n",
    "    for pattern in weight_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                features['weight'] = float(match.group(1))\n",
    "            except:\n",
    "                features['weight'] = 0.0\n",
    "            break\n",
    "    \n",
    "    # Extract volume\n",
    "    volume_patterns = [\n",
    "        r'(\\d+\\.?\\d*)\\s*fl\\s*oz',\n",
    "        r'(\\d+\\.?\\d*)\\s*fluid\\s*ounce',\n",
    "        r'(\\d+\\.?\\d*)\\s*ml',\n",
    "        r'(\\d+\\.?\\d*)\\s*liter'\n",
    "    ]\n",
    "    for pattern in volume_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                features['volume'] = float(match.group(1))\n",
    "            except:\n",
    "                features['volume'] = 0.0\n",
    "            break\n",
    "    \n",
    "    # Extract count\n",
    "    count_patterns = [\n",
    "        r'(\\d+)\\s*tea bags',\n",
    "        r'(\\d+)\\s*capsules',\n",
    "        r'(\\d+)\\s*pods',\n",
    "        r'(\\d+)\\s*cookies'\n",
    "    ]\n",
    "    for pattern in count_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                features['count'] = float(match.group(1))\n",
    "            except:\n",
    "                features['count'] = 0.0\n",
    "            break\n",
    "    \n",
    "    # Extract unit from the structured Unit field\n",
    "    unit_match = re.search(r'unit:\\s*([^\\n]+)', catalog_text, re.IGNORECASE)\n",
    "    if unit_match:\n",
    "        features['unit'] = unit_match.group(1).strip().lower()\n",
    "    \n",
    "    # Extract flavor information\n",
    "    flavor_keywords = ['vanilla', 'chocolate', 'strawberry', 'lemon', 'mint', 'berry', \n",
    "                      'caramel', 'honey', 'spice', 'cinnamon', 'ginger', 'peach']\n",
    "    for flavor in flavor_keywords:\n",
    "        if flavor in text:\n",
    "            features['flavor'] = flavor\n",
    "            break\n",
    "    \n",
    "    # Extract brand names\n",
    "    brand_patterns = [\n",
    "        r'manufacturer:\\s*([^\\n]+)',\n",
    "        r'brand:\\s*([^\\n]+)'\n",
    "    ]\n",
    "    for pattern in brand_patterns:\n",
    "        match = re.search(pattern, catalog_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            features['brand'] = match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # Health and dietary attributes\n",
    "    features['is_organic'] = 1 if any(term in text for term in ['organic', 'usda organic']) else 0\n",
    "    features['is_gluten_free'] = 1 if 'gluten free' in text else 0\n",
    "    features['is_vegan'] = 1 if 'vegan' in text else 0\n",
    "    features['is_kosher'] = 1 if 'kosher' in text else 0\n",
    "    features['is_sugar_free'] = 1 if any(term in text for term in ['sugar free', 'no sugar', 'zero sugar']) else 0\n",
    "    features['is_low_carb'] = 1 if any(term in text for term in ['low carb', 'keto', 'keto-friendly']) else 0\n",
    "    features['is_non_gmo'] = 1 if any(term in text for term in ['non-gmo', 'non gmo']) else 0\n",
    "    \n",
    "    # Extract nutritional information\n",
    "    calorie_match = re.search(r'(\\d+)\\s*calories', text)\n",
    "    if calorie_match:\n",
    "        try:\n",
    "            features['calories_per_serving'] = float(calorie_match.group(1))\n",
    "        except:\n",
    "            features['calories_per_serving'] = 0.0\n",
    "    \n",
    "    protein_match = re.search(r'(\\d+\\.?\\d*)\\s*g\\s*protein', text)\n",
    "    if protein_match:\n",
    "        try:\n",
    "            features['protein_content'] = float(protein_match.group(1))\n",
    "        except:\n",
    "            features['protein_content'] = 0.0\n",
    "    \n",
    "    fiber_match = re.search(r'(\\d+\\.?\\d*)\\s*g\\s*fiber', text)\n",
    "    if fiber_match:\n",
    "        try:\n",
    "            features['fiber_content'] = float(fiber_match.group(1))\n",
    "        except:\n",
    "            features['fiber_content'] = 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_derived_features(df, has_price=True):\n",
    "    \"\"\"\n",
    "    Create derived features, handling the case where price might be missing\n",
    "    \"\"\"\n",
    "    # Product category based on item name\n",
    "    def categorize_product(item_name):\n",
    "        if pd.isna(item_name):\n",
    "            return 'other'\n",
    "        item_name = str(item_name).lower()\n",
    "        if any(word in item_name for word in ['tea', 'chai']):\n",
    "            return 'tea'\n",
    "        elif any(word in item_name for word in ['coffee', 'brew']):\n",
    "            return 'coffee'\n",
    "        elif any(word in item_name for word in ['snack', 'chip', 'cracker', 'cookie']):\n",
    "            return 'snack'\n",
    "        elif any(word in item_name for word in ['sauce', 'dressing', 'oil']):\n",
    "            return 'condiment'\n",
    "        elif any(word in item_name for word in ['spice', 'seasoning']):\n",
    "            return 'spice'\n",
    "        elif any(word in item_name for word in ['candy', 'chocolate']):\n",
    "            return 'candy'\n",
    "        elif any(word in item_name for word in ['pasta', 'rice', 'grain']):\n",
    "            return 'grain'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    df['product_category'] = df['item_name'].apply(categorize_product)\n",
    "    \n",
    "    # Text length features\n",
    "    df['catalog_content_length'] = df['catalog_content'].str.len().fillna(0)\n",
    "    \n",
    "    # Only create price-related features if price is available\n",
    "    if has_price and 'price' in df.columns:\n",
    "        if 'weight' in df.columns:\n",
    "            weight_safe = df['weight'].replace(0, np.nan)\n",
    "            df['price_per_oz'] = df['price'] / weight_safe\n",
    "            df['price_per_oz'] = df['price_per_oz'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        if 'volume' in df.columns:\n",
    "            volume_safe = df['volume'].replace(0, np.nan)\n",
    "            df['price_per_fl_oz'] = df['price'] / volume_safe\n",
    "            df['price_per_fl_oz'] = df['price_per_fl_oz'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        if 'count' in df.columns:\n",
    "            count_safe = df['count'].replace(0, np.nan)\n",
    "            df['price_per_count'] = df['price'] / count_safe\n",
    "            df['price_per_count'] = df['price_per_count'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ========== MAIN PREDICTOR CLASS ==========\n",
    "\n",
    "class RobustSMAPEStackingPricePredictor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.feature_columns = []\n",
    "        self.expected_columns = []\n",
    "        self.stacking_model = None\n",
    "        self.is_trained = False\n",
    "        self.best_params_ = None\n",
    "        \n",
    "    def preprocess_data(self, df, is_training=True):\n",
    "        \"\"\"\n",
    "        Preprocess the data: extract features, handle missing values, encode categorical variables\n",
    "        \"\"\"\n",
    "        print(\"Step 1: Extracting features from catalog content...\")\n",
    "        \n",
    "        # Make a copy to avoid modifying original data\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Extract features\n",
    "        catalog_features = df_processed['catalog_content'].apply(extract_features_from_catalog)\n",
    "        features_df = pd.DataFrame(catalog_features.tolist(), index=df_processed.index)\n",
    "        \n",
    "        # Concatenate with original data\n",
    "        df_enhanced = pd.concat([df_processed, features_df], axis=1)\n",
    "        \n",
    "        # Ensure all expected numerical columns exist with proper defaults\n",
    "        numerical_columns = ['weight', 'volume', 'count', 'calories_per_serving', 'protein_content', 'fiber_content']\n",
    "        for col in numerical_columns:\n",
    "            if col not in df_enhanced.columns:\n",
    "                df_enhanced[col] = 0.0\n",
    "            else:\n",
    "                df_enhanced[col] = pd.to_numeric(df_enhanced[col], errors='coerce').fillna(0.0)\n",
    "        \n",
    "        # Ensure all expected boolean columns exist\n",
    "        boolean_columns = ['is_organic', 'is_gluten_free', 'is_vegan', 'is_kosher', \n",
    "                          'is_sugar_free', 'is_low_carb', 'is_non_gmo']\n",
    "        for col in boolean_columns:\n",
    "            if col not in df_enhanced.columns:\n",
    "                df_enhanced[col] = 0\n",
    "            else:\n",
    "                df_enhanced[col] = df_enhanced[col].astype(int)\n",
    "        \n",
    "        # Handle categorical columns\n",
    "        categorical_columns = ['unit', 'flavor', 'brand', 'item_name']\n",
    "        for col in categorical_columns:\n",
    "            if col not in df_enhanced.columns:\n",
    "                df_enhanced[col] = 'unknown'\n",
    "            else:\n",
    "                df_enhanced[col] = df_enhanced[col].fillna('unknown')\n",
    "        \n",
    "        # Create derived features\n",
    "        has_price = is_training and 'price' in df_enhanced.columns\n",
    "        df_enhanced = create_derived_features(df_enhanced, has_price=has_price)\n",
    "        \n",
    "        # Handle outliers in price if it exists and we're training\n",
    "        if is_training and 'price' in df_enhanced.columns:\n",
    "            Q1 = df_enhanced['price'].quantile(0.25)\n",
    "            Q3 = df_enhanced['price'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = max(0, Q1 - 1.5 * IQR)  # Ensure lower bound is not negative\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            df_enhanced['price'] = np.where(df_enhanced['price'] > upper_bound, upper_bound, df_enhanced['price'])\n",
    "            df_enhanced['price'] = np.where(df_enhanced['price'] < lower_bound, lower_bound, df_enhanced['price'])\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    def prepare_features(self, df, is_training=True):\n",
    "        \"\"\"\n",
    "        Prepare features for modeling with robust error handling\n",
    "        \"\"\"\n",
    "        # Define core features that should always be present\n",
    "        core_features = [\n",
    "            'weight', 'volume', 'count', 'catalog_content_length',\n",
    "            'is_organic', 'is_gluten_free', 'is_vegan', 'is_kosher', \n",
    "            'is_sugar_free', 'is_low_carb', 'is_non_gmo'\n",
    "        ]\n",
    "        \n",
    "        # Add price per unit features if available (only in training)\n",
    "        if is_training:\n",
    "            price_per_features = [col for col in df.columns if col.startswith('price_per')]\n",
    "            feature_columns = core_features + price_per_features\n",
    "        else:\n",
    "            feature_columns = core_features.copy()\n",
    "        \n",
    "        # Ensure all feature columns exist in the dataframe\n",
    "        missing_features = set(feature_columns) - set(df.columns)\n",
    "        for feature in missing_features:\n",
    "            df[feature] = 0.0  # Add missing features with default value\n",
    "        \n",
    "        # Select only the feature columns we want\n",
    "        feature_columns = [col for col in feature_columns if col in df.columns]\n",
    "        X = df[feature_columns].copy()\n",
    "        \n",
    "        # Replace infinite values with NaN\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        if is_training:\n",
    "            self.feature_columns = feature_columns\n",
    "            self.expected_columns = feature_columns.copy()\n",
    "            \n",
    "            # Fit imputer and scaler on training data\n",
    "            X_imputed = self.imputer.fit_transform(X)\n",
    "            \n",
    "            # Scale numerical features\n",
    "            numerical_cols = [col for col in X.columns if X[col].dtype in ['float64', 'float32', 'int64']]\n",
    "            if numerical_cols:\n",
    "                X_imputed = self.scaler.fit_transform(X_imputed)\n",
    "            \n",
    "            X_processed = pd.DataFrame(X_imputed, columns=feature_columns, index=X.index)\n",
    "            \n",
    "            if 'price' in df.columns:\n",
    "                y = df['price'].copy()\n",
    "                return X_processed, y\n",
    "            else:\n",
    "                return X_processed\n",
    "        else:\n",
    "            # Ensure we have the same columns as training\n",
    "            missing_cols = set(self.expected_columns) - set(X.columns)\n",
    "            extra_cols = set(X.columns) - set(self.expected_columns)\n",
    "            \n",
    "            for col in missing_cols:\n",
    "                X[col] = 0.0\n",
    "            for col in extra_cols:\n",
    "                X = X.drop(col, axis=1)\n",
    "            \n",
    "            # Reorder columns to match training\n",
    "            X = X[self.expected_columns]\n",
    "            \n",
    "            # Transform using fitted imputer and scaler\n",
    "            X_imputed = self.imputer.transform(X)\n",
    "            X_imputed = self.scaler.transform(X_imputed)\n",
    "            \n",
    "            X_processed = pd.DataFrame(X_imputed, columns=self.expected_columns, index=X.index)\n",
    "            return X_processed\n",
    "    \n",
    "    def build_stacking_regressor(self):\n",
    "        \"\"\"\n",
    "        Build a StackingRegressor with multiple base models\n",
    "        \"\"\"\n",
    "        # Base models with robust parameters\n",
    "        base_models = [\n",
    "            ('ridge', Ridge(alpha=1.0, random_state=42)),\n",
    "            ('lasso', Lasso(alpha=0.1, random_state=42)),\n",
    "            ('rf', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "            ('gbm', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "            ('svr', SVR(kernel='rbf', C=1.0))\n",
    "        ]\n",
    "        \n",
    "        # Meta-model\n",
    "        meta_model = LinearRegression()\n",
    "        \n",
    "        # Create StackingRegressor\n",
    "        stacking_regressor = StackingRegressor(\n",
    "            estimators=base_models,\n",
    "            final_estimator=meta_model,\n",
    "            cv=5,\n",
    "            passthrough=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return stacking_regressor\n",
    "    \n",
    "    def _calculate_aggregated_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Calculate feature importance by aggregating importances from all base models\n",
    "        \"\"\"\n",
    "        # Initialize importance array\n",
    "        aggregated_importance = np.zeros(len(self.feature_columns))\n",
    "        \n",
    "        # Get importances from each base model\n",
    "        for name, model in self.stacking_model.estimators_:\n",
    "            try:\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    # Tree-based models (RandomForest, GradientBoosting)\n",
    "                    importance = model.feature_importances_\n",
    "                elif hasattr(model, 'coef_'):\n",
    "                    # Linear models (Ridge, Lasso)\n",
    "                    importance = abs(model.coef_)\n",
    "                else:\n",
    "                    # Models without feature importance (SVR) - skip\n",
    "                    continue\n",
    "                \n",
    "                # Ensure the importance array matches our feature dimensions\n",
    "                if len(importance) == len(self.feature_columns):\n",
    "                    aggregated_importance += importance\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not get importance from {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize the importance scores\n",
    "        if aggregated_importance.sum() > 0:\n",
    "            aggregated_importance = aggregated_importance / aggregated_importance.sum()\n",
    "        \n",
    "        # Create DataFrame\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': self.feature_columns,\n",
    "            'importance': aggregated_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return feature_importance_df\n",
    "    \n",
    "    def train(self, train_df, perform_tuning=True):\n",
    "        print(\"Starting SMAPE-tuned StackingRegressor training...\")\n",
    "\n",
    "        # Preprocess training data\n",
    "        df_processed = self.preprocess_data(train_df, is_training=True)\n",
    "\n",
    "        # Prepare features\n",
    "        X, y = self.prepare_features(df_processed, is_training=True)\n",
    "\n",
    "        print(f\"Training data shape: {X.shape}\")\n",
    "        print(f\"Target variable shape: {y.shape}\")\n",
    "        print(f\"Features used: {len(self.feature_columns)}\")\n",
    "\n",
    "        # Split data for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Build stacking regressor\n",
    "        stacking_model = self.build_stacking_regressor()\n",
    "\n",
    "        if perform_tuning:\n",
    "            print(\"Performing randomized hyperparameter tuning with SMAPE scoring...\")\n",
    "\n",
    "            # Define hyperparameter search space\n",
    "            param_distributions = {\n",
    "                'ridge__alpha': np.logspace(-2, 2, 10),\n",
    "                'lasso__alpha': np.logspace(-3, 1, 10),\n",
    "                'rf__n_estimators': np.arange(50, 300, 50),\n",
    "                'rf__max_depth': [None, 5, 10, 15, 20],\n",
    "                'gbm__n_estimators': np.arange(50, 300, 50),\n",
    "                'gbm__learning_rate': np.linspace(0.01, 0.2, 10),\n",
    "                'svr__C': np.logspace(-2, 2, 10),\n",
    "                'svr__gamma': ['scale', 'auto']\n",
    "            }\n",
    "\n",
    "            # Perform RandomizedSearchCV with SMAPE\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=stacking_model,\n",
    "                param_distributions=param_distributions,\n",
    "                n_iter=20,\n",
    "                scoring=smape_scorer,\n",
    "                cv=3,\n",
    "                verbose=2,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            random_search.fit(X_train, y_train)\n",
    "\n",
    "            # Best model and parameters\n",
    "            self.stacking_model = random_search.best_estimator_\n",
    "            self.best_params_ = random_search.best_params_\n",
    "\n",
    "            print(f\"Best parameters found: {self.best_params_}\")\n",
    "\n",
    "        else:\n",
    "            print(\"Training without hyperparameter tuning...\")\n",
    "            self.stacking_model = stacking_model\n",
    "            self.stacking_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model on validation set\n",
    "        y_pred = self.stacking_model.predict(X_val)\n",
    "\n",
    "        val_smape = smape(y_val, y_pred)\n",
    "        val_mae = mean_absolute_error(y_val, y_pred)\n",
    "        val_mse = mean_squared_error(y_val, y_pred)\n",
    "        val_r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "        print(\"\\n=== VALIDATION PERFORMANCE ===\")\n",
    "        print(f\"SMAPE: {val_smape:.4f}%\")\n",
    "        print(f\"MAE: ${val_mae:.2f}\")\n",
    "        print(f\"MSE: ${val_mse:.2f}\")\n",
    "        print(f\"R²: {val_r2:.4f}\")\n",
    "\n",
    "        # Cross-validation SMAPE\n",
    "        cv_scores = cross_val_score(\n",
    "            self.stacking_model, X, y, cv=3, scoring=smape_scorer, n_jobs=-1\n",
    "        )\n",
    "        cv_smape_scores = -cv_scores  # Convert back to positive values\n",
    "\n",
    "        print(f\"\\nCross-validation SMAPE: {cv_smape_scores.mean():.4f}% (+/- {cv_smape_scores.std() * 2:.4f}%)\")\n",
    "\n",
    "        # Retrain on full dataset\n",
    "        print(\"\\nRetraining on full training dataset...\")\n",
    "        self.stacking_model.fit(X, y)\n",
    "        self.is_trained = True\n",
    "\n",
    "        # FIXED: Proper feature importance for stacking regressor\n",
    "        print(\"\\n=== STACKING MODEL ANALYSIS ===\")\n",
    "        \n",
    "        # Show base model weights in the meta-model\n",
    "        if hasattr(self.stacking_model.final_estimator_, 'coef_'):\n",
    "            base_model_names = [name for name, _ in self.stacking_model.estimators_]\n",
    "            meta_coefficients = abs(self.stacking_model.final_estimator_.coef_)\n",
    "            \n",
    "            # Create a DataFrame showing base model importance in the meta-model\n",
    "            meta_importance = pd.DataFrame({\n",
    "                'base_model': base_model_names,\n",
    "                'meta_weight': meta_coefficients\n",
    "            }).sort_values('meta_weight', ascending=False)\n",
    "            \n",
    "            print(\"Base Model Weights in Meta-Model:\")\n",
    "            print(meta_importance)\n",
    "        \n",
    "        # Calculate aggregated feature importance from base models\n",
    "        try:\n",
    "            feature_importance = self._calculate_aggregated_feature_importance()\n",
    "            print(\"\\nTop 10 Most Important Features (Aggregated from Base Models):\")\n",
    "            print(feature_importance.head(10))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nNote: Feature importance calculation skipped: {e}\")\n",
    "\n",
    "        return {\n",
    "            'smape': val_smape,\n",
    "            'mae': val_mae,\n",
    "            'mse': val_mse,\n",
    "            'r2': val_r2,\n",
    "            'cv_smape_mean': cv_smape_scores.mean(),\n",
    "            'cv_smape_std': cv_smape_scores.std()\n",
    "        }\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        \"\"\"\n",
    "        Predict prices for test data using the trained StackingRegressor\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        print(\"Making predictions on test data...\")\n",
    "        \n",
    "        # Preprocess test data\n",
    "        df_processed = self.preprocess_data(test_df, is_training=False)\n",
    "        \n",
    "        # Prepare features\n",
    "        X_test = self.prepare_features(df_processed, is_training=False)\n",
    "        \n",
    "        print(f\"Test data shape after preprocessing: {X_test.shape}\")\n",
    "        print(f\"Expected features: {len(self.expected_columns)}\")\n",
    "        print(f\"Actual features: {len(X_test.columns)}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.stacking_model.predict(X_test)\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results = test_df.copy()\n",
    "        results['predicted_price'] = predictions\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model and preprocessors\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"No trained model to save\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        model_data = {\n",
    "            'scaler': self.scaler,\n",
    "            'imputer': self.imputer,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'expected_columns': self.expected_columns,\n",
    "            'stacking_model': self.stacking_model,\n",
    "            'best_params': self.best_params_,\n",
    "            'is_trained': self.is_trained\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"SMAPE-tuned StackingRegressor saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Load a trained model and preprocessors\n",
    "        \"\"\"\n",
    "        model_data = joblib.load(filepath)\n",
    "        \n",
    "        self.scaler = model_data['scaler']\n",
    "        self.imputer = model_data['imputer']\n",
    "        self.feature_columns = model_data['feature_columns']\n",
    "        self.expected_columns = model_data['expected_columns']\n",
    "        self.stacking_model = model_data['stacking_model']\n",
    "        self.best_params_ = model_data['best_params']\n",
    "        self.is_trained = model_data['is_trained']\n",
    "        \n",
    "        print(f\"SMAPE-tuned StackingRegressor loaded from {filepath}\")\n",
    "        if self.best_params_:\n",
    "            print(f\"Best parameters: {self.best_params_}\")\n",
    "\n",
    "# ========== MAIN EXECUTION PIPELINE ==========\n",
    "\n",
    "def run_complete_pipeline(train_file='../data/train.csv', test_file='../data/test.csv'):\n",
    "    \"\"\"\n",
    "    Complete pipeline with robust error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"=== AMAZON PRODUCT PRICE PREDICTION PIPELINE ===\\n\")\n",
    "        \n",
    "        # Step 1: Load training data (with prices)\n",
    "        print(\"1. Loading training data...\")\n",
    "        train_df = pd.read_csv(train_file)\n",
    "        \n",
    "        print(f\"Training data shape: {train_df.shape}\")\n",
    "        print(\"Training columns:\", train_df.columns.tolist())\n",
    "        \n",
    "        if 'price' not in train_df.columns:\n",
    "            raise ValueError(\"Training data must contain 'price' column\")\n",
    "        \n",
    "        missing_prices = train_df['price'].isnull().sum()\n",
    "        print(f\"Available prices in training data: {len(train_df) - missing_prices}\")\n",
    "        print(f\"Missing prices in training data: {missing_prices}\")\n",
    "        \n",
    "        # Step 2: Load test data (without prices)\n",
    "        print(\"\\n2. Loading test data...\")\n",
    "        test_df = pd.read_csv(test_file)\n",
    "        \n",
    "        print(f\"Test data shape: {test_df.shape}\")\n",
    "        print(\"Test columns:\", test_df.columns.tolist())\n",
    "        \n",
    "        if 'price' in test_df.columns:\n",
    "            if test_df['price'].isnull().all():\n",
    "                print(\"Test data contains 'price' column with all null values - will ignore for predictions\")\n",
    "                test_df = test_df.drop('price', axis=1)\n",
    "            else:\n",
    "                print(\"Warning: Test data contains non-null 'price' column\")\n",
    "        \n",
    "        # Step 3: Initialize and train the model\n",
    "        print(\"\\n3. Training SMAPE-tuned StackingRegressor...\")\n",
    "        smape_predictor = RobustSMAPEStackingPricePredictor()\n",
    "        \n",
    "        # Train with hyperparameter tuning\n",
    "        training_results = smape_predictor.train(train_df, perform_tuning=True)\n",
    "        \n",
    "        # Step 4: Save the trained model\n",
    "        print(\"\\n4. Saving trained model...\")\n",
    "        smape_predictor.save_model('../Model/robust_smape_stacking_predictor.pkl')\n",
    "        \n",
    "        # Step 5: Make predictions on test data\n",
    "        print(\"\\n5. Making predictions on test data...\")\n",
    "        test_predictions = smape_predictor.predict(test_df)\n",
    "        \n",
    "        print(\"Prediction completed!\")\n",
    "        print(f\"Predicted prices for {len(test_predictions)} test products\")\n",
    "        \n",
    "        # Step 6: Display prediction results\n",
    "        print(\"\\n6. Prediction results:\")\n",
    "        print(test_predictions[['sample_id', 'predicted_price']].head(10))\n",
    "        \n",
    "        print(\"\\nTest Prediction Statistics:\")\n",
    "        print(test_predictions['predicted_price'].describe())\n",
    "        \n",
    "        # Step 7: Save results\n",
    "        print(\"\\n7. Saving results...\")\n",
    "        # Create Model directory if it doesn't exist\n",
    "        os.makedirs('../Model', exist_ok=True)\n",
    "        test_predictions[['sample_id', 'predicted_price']].to_csv('../Model/final_test_predictions.csv', index=False)\n",
    "        print(\"Predictions saved to '../Model/final_test_predictions.csv'\")\n",
    "        \n",
    "        return smape_predictor, test_predictions, training_results\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found error: {e}\")\n",
    "        print(\"Please check that the data files exist in the specified paths\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error in pipeline: {str(e)}\")\n",
    "        print(\"Traceback:\", traceback.format_exc())\n",
    "        return None, None, None\n",
    "\n",
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    predictor, predictions, results = run_complete_pipeline('../data/train.csv', '../data/test.csv')\n",
    "    \n",
    "    if predictor is not None:\n",
    "        print(\"\\n=== PIPELINE COMPLETED SUCCESSFULLY ===\")\n",
    "        print(f\"Final SMAPE: {results['smape']:.4f}%\")\n",
    "        print(f\"Final R²: {results['r2']:.4f}\")\n",
    "    else:\n",
    "        print(\"\\n=== PIPELINE FAILED ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playstore-dataset-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
