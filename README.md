# Amazon Product Price Prediction: A Journey to 25.84% SMAPE

## Project Overview :

-This project aims to predict the price of Amazon products based on their catalog descriptions and images. The primary goal is to minimize the Symmetric Mean Absolute Percentage Error (SMAPE), a metric that is robust to outliers and particularly suited for price prediction tasks. The dataset consists of 50,000 training samples with prices and 25,000 test samples without prices, requiring the model to generalize well to unseen data.

### Journey

- At first after data cleaning and feature engineering into 14 useful columns and by applying 4 different models we got a **SMAPE score of 34.80** on the csv file lightning_prediction.csv file (model used to predict here is RandomForestRegressor).

- After confirming that we have reached the level best score of 34.80 , changing the model frm RandomForestRegressor to GradientBoostingRegressor gave us a number lower by 0.72 bringing our **SMAPE score to 34.08**. (name of the optimized_prediction_optimized.csv)

- But in the same notebook trying an ensemble technique where 4 different models like GradientBoostingRegressor , RandomForestregressor , Ridge and ExtraTreeRegressor gave us a **SMAPE score of 34.47**. (name of the file is optimized_prediction_ensemble.csv)

- To break the final score lowest score of 34.08 my teammate (Gourav Saha) suggested to use NLP features (NLTK) for the categorical data along with all the boosting algorithms such as Catboost , xgboost , lightgbm and GradientBoostingRegressor breaking the minimum score of ~34 to a **SMAPE score of 26.16**.

- Along with all the base models ( GradBoostRegressor , lgbm, catboost , xgboost) a metamodel was kept in place which is trained using the predictions generated by the base models as its input features.

- Another major breakthrough came when the hyperparameters namely n_estimators(5000 -> 500) and the leanrning rate(0.005 -> 0.5) were changed drastically giving us a **SMAPE score of 25.84** (name of the file optimized_submission.csv)

- Another thought provoking experiment was thinking that GradientBoostingRegressor being the new metamodel can lower the SMAPE score than 25.84 . But sadly it generated a **SMAPE score of 25.85**.(name of the file gbm_meta_submission.csv)

- The unfinished version would be to use image analysis for bringing down the SMAPE score . A new paradigm for analyzing the dataset but unfiinised on our implementation.
